{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2\n",
    "In this section, we'll build off the contents of Section 1 and introduce cross-validation, evaluation metrics, and demonstrate some useful visualizations to better understand your model performance.\n",
    "\n",
    "# Definitions\n",
    "Here's a summary of terms used in this section. They're provided here as an easy reference.\n",
    "  \n",
    "|Term|Definition|\n",
    "|:--|:--|\n",
    "|**Training set**|A subset of the data that is used to directly inform model parameters.|\n",
    "|**Validation set**|A subset of the data that is used to compare trained models. The data indirectly informs model parameters by selecting the best-performing set of parameters.|\n",
    "|**Test set**|A subset of the data the is used to predict model performance on unseen data.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up matplotlib to be inline:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Set\n",
    "At the end of Section 1, we (intentionally) broke one of the rules for handling datasets: we used the test set multiple times. The purpose of the test set is to have data that has not been used to inform our model parameters. By selecting a model based on test set results, we're implicitly informing our parameters.  \n",
    "  \n",
    "In addition to the training and test sets, we introduce a third set called the validation set which we use for model comparison.  \n",
    "![Validation Set](img/dataset_schematic.png)  \n",
    "  \n",
    "Note: The use of a validation set is ubiquitous; its presence is not always explicitly named, and is often assumed to be a subset of the training set.  \n",
    "  \n",
    "We use the validation set to compare different models and to select which one we _expect_ to perform best on unseen data (i.e., which one generalizes best). As with the training set, performing well on the validation set is not an actual evaluation of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "Cross-validation is the use of different subsets of the training data to train and evaluate model performance. When doing cross-validation, the training and validation sets are not predefined:  \n",
    "![Cross Validation](img/cross_validation.png)  \n",
    "  \n",
    "Cross-validation has a number of benefits. It provides some protection against our validation data being 'lucky'; if one subset happens to be easy for a model, we probably don't want to select that model based on just that subset. By using multiple different splits for the training and validation sets, it's less likely for that to be an issue. Cross-validation provides us with a distribution for model performance instead of a single performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross-validation\n",
    "A common method of cross-validation is K-fold cross-validation. In K-fold cross-validation, you split your training data into K equally-sized subsets. You set aside one subset as your validation set and train your model on the remaining (K-1) subsets. You repeat this step for each subset until you've trained K different models. It can be visualized as the following:\n",
    "![Kfold crossvalidation](img/kfold.png)  \n",
    "  \n",
    "The diagram demonstrates 5-fold crossvalidation. When using K-fold, you'll typically want to use a value for K between 5 and 10. Too few folds results in a lot of the data being unused for validation, while too many folds gives estimators that are very similar to one another. See the [sklearn documentation](https://scikit-learn.org/stable/modules/cross_validation.html#k-fold) for additional information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Exercise 1\n",
    "\n",
    "Let's see how to use K-fold cross validation in `sklearn`. As you may have guessed, `sklearn` has a function for creating the different folds for Kfold crossvalidation: `sklearn.model_selection.KFold`.  \n",
    "  \n",
    "However, understanding how to use `KFold` requires a bit of Python background. In the last section, we dealt with a number of `objects`: our models were objects, as was `PolynomialFeatures`.  \n",
    "An object is conceptually similar to a variable: it must be initialized and it is used to store information. For example, the variable `a` doesn't make sense alone, but if you first have `a=5`, the variable now stores the data `5`.  \n",
    "_Unlike_ variables, objects can also store `methods`, which are functions that use the object's data.  \n",
    "  \n",
    "There is a type of object in Python called a _generator_, which returns values when put in a `for` loop. For our context:  \n",
    "1) `KFold` returns an object. `kf = KFold(n_splits=2)` will store the object in `kf`  \n",
    "2) The object `kf` has a method called `split`  \n",
    "3) The method `kf.split(x, y)` returns a generator. The generator gives array indices for the training and validation samples.  \n",
    "4) We can use `kf.split(x, y)` in a `for` loop to generate K folds.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "x = np.arange(20,30)  # Create easy-to-recognize data (20-29)\n",
    "y = np.arange(40,50)  # Create easy-to-recognize data (40-49)\n",
    "kf = KFold(n_splits=2)  # Change the number of n_splits \n",
    "for train_ind, valid_ind in kf.split(x, y):\n",
    "    print('train_data: ' + str(x[train_ind]))\n",
    "    print('train_label: ' + str(y[train_ind]))\n",
    "    print('valid_data: ' + str(x[valid_ind]))\n",
    "    print('valid_label: ' + str(y[valid_ind]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a practical example of KFold crossvalidation. We'll use one of `sklearn`'s sample datasets instead of our previous polynomials. Load the diabetes dataset using `sklearn.datasets.load_diabetes`. The features are described in the [documentation](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#help(sklearn.datasets.load_diabetes)\n",
    "d_data, d_label = sklearn.datasets.load_diabetes(return_X_y=True)\n",
    "print(d_data.shape)\n",
    "print(d_label.shape)\n",
    "d_data_train, d_data_test, d_label_train, d_label_test = train_test_split(d_data, d_label, train_size=0.8)\n",
    "print(d_data_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a similar structure `for` loop structure above, perform 5-fold crossvalidation:  \n",
    "1) Create a 'kf' object with 5 splits.  \n",
    "2) Put the 'kf' object in a loop and split the data and the labels.  \n",
    "3) In each iteration of the loop, initialize and train a new model.  \n",
    "4) Save the R2 score of each trained model.  \n",
    "5) Print out the average score  \n",
    "6) Plot the scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages you'll need\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Create the kf object\n",
    "\n",
    "\n",
    "# Initialize\n",
    "r2_score_list = []\n",
    "# loop\n",
    "coef_list = []\n",
    "for train_ind, valid_ind in kf.split(d_data_train, d_label_train):\n",
    "    ## Set your data/labels:\n",
    "    train_data = d_data_train[train_ind,:]\n",
    "    #train_label = \n",
    "    \n",
    "    # set your validation:\n",
    "    # valid_data\n",
    "    # valid_label\n",
    "    \n",
    "    # initialize and fit your model\n",
    "    \n",
    "    \n",
    "    # Get score\n",
    "    # r2_score_list.append(\n",
    "\n",
    "# Plot score\n",
    "print(r2_score_list)\n",
    "print('Mean R2: ' + str(np.mean(r2_score_list)))\n",
    "plt.bar(np.arange(5),r2_score_list)\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('R2 Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the variation between each fold even though the structure of the model is the same. If two people were given shuffled versions of the set and neither did cross-validation, one would report an R2 score of `0.40` while the other would report `0.53`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Exercise 2 (Simplified!)\n",
    "\n",
    "The previous exercise is intended to walk you through what is happening in K-fold cross-validation. In practice, it would be tedious to go through each of those steps. As a general rule: if you find yourself repeatedly doing something with `sklearn`, there's probably a helper function that will make your life easier.  \n",
    "  \n",
    "Since cross-validation is central to machine learning, there's a few helper functions to help you. For the previous example, we could use: `sklearn.model_selection.cross_val_score`.  \n",
    "Let's take a look at its documentation; identify the parameters that will be useful for us, and see what the function returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "help(sklearn.model_selection.cross_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of interest for us are `estimator`, `X`, `y`, and `cv`. Using what you saw in the previous exercise, write the code needed for 5-fold cross-validation:  \n",
    "1) Save the R2 score of each trained model.  \n",
    "2) Print out the average score  \n",
    "3) Plot the scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules you'll need\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Get the data and split into train/test\n",
    "d_data, d_label = sklearn.datasets.load_diabetes(return_X_y=True)\n",
    "d_data_train, d_data_test, d_label_train, d_label_test = train_test_split(d_data, d_label, train_size=0.8)\n",
    "\n",
    "# Initialize our model\n",
    "mdl = LinearRegression()\n",
    "\n",
    "### Use cross_cal_score to get the list of scores\n",
    "\n",
    "### Print the list of scores and its mean:  \n",
    "\n",
    "\n",
    "plt.bar(np.arange(5), score_list)\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Estimator Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll probably notice that these results aren't the same as the ones we saw in the previous exercise. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other cross-validation methods\n",
    "There are other types of cross-validation. For example, \"Leave-p-out cross-validation\" selects `p` samples for validation, and trains the model on the remaining samples. At every iteration, a different set of `p` samples is selected. Iterations continue until all possible permutations are considered. In the extreme case where `p=1`, we're left with Leave-one-out cross-validation.\n",
    "If we want to use a method of cross-validation that isn't K-fold, we just need to specify it in `cross_val_score` using the `cv` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the cross-validator you want to use\n",
    "from sklearn.model_selection import LeavePOut\n",
    "# Look at the documentation:\n",
    "# help(LeaveOneOut)\n",
    "# Initialize\n",
    "leave_p_out = LeavePOut(2)\n",
    "\n",
    "# NOTE:\n",
    "# LeavePOut is unwieldy in practice since the number of permutations grows combinatorially (n choose p)\n",
    "# with dataset size. For that reason, LeaveOneOut is \n",
    "\n",
    "# Replace the value for 'cv' by our cross-validator:\n",
    "# score_list = cross_val_score([fill_this_in], cv=leave_p_out)\n",
    "# print(len(score_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're looking to have behaviour similar to `LeavePOut` without the combinatorial issues, you can use `ShuffleSplit` in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cross-validator\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Look at the docs\n",
    "# help(ShuffleSplit)\n",
    "shuff = ShuffleSplit(n_splits=100, train_size=0.8)\n",
    "\n",
    "# Note the 'cv' parameter here is changed to the cross validation object we just created\n",
    "# Fill in the missing information using the variables you just used:\n",
    "\n",
    "# score_list = cross_val_score( [fill this in], cv=shuff)\n",
    "# print(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Evaluation\n",
    "Cross-validation is not only used to evaluate models. Its parent module, `model_selection`, hints at that. One of the reasons for splitting a dataset between training, validation, and testing sets is to avoid having the testing set implicitly inform our model parameters.  \n",
    "  \n",
    "Suppose you initialize a linear regression model with random coefficients, then compared them to one another. Simply by chance, some of them would have parameters that would make them perform better. Selecting one of these is a simple kind of training.  \n",
    "  \n",
    "In the previous exercises, we saw cross-validation used for the same type of models, but on different subsets of data. We can perform cross-validation across more than just parameter values; we could do it across different types of model and then select whichever model works best. We won't need to choose _a priori_ whether to use linear regression, a support vector regressor, or whether to take polynomials of our features. We can do cross-validation to compare the performance of these models and then select based on an objective measure of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation wrap-up\n",
    "At its core, cross-validation is simply about making sure that we evaluate our models fairly so that the performance we claim is accurate. With a fair comparison, we can empirically evaluate which models are best without having to worry about our own biases affecting our choices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
