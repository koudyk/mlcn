{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression vs. Classification?\n",
    "\n",
    "In the previous sections, we restricted our examples to data that would be appropriate for linear regression models. That is, our labels were real-valued; a value of `4` was more than `3.3` and less than `4.5`. If our model made a prediction of `3.9`, we knew it was a better prediction that if it had predicted `2.9`. When labels are real-valued and ordered like this, the problem is known as a **regression** problem.  \n",
    "  \n",
    "The other type of problem is **classification**, where we are trying to assign a sample to a particular group (i.e., classify it). In medical applications, you may want your model to produce a diagnosis or classify a tissue type. A **classifier** is a model that produces a categorical label for a input samples. A variable whose values corresponds to different classes or categories is a **categorical** variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers\n",
    "Let's create and train a classifier. We'll use a relatively-simple classifier that has an unfortunate name: `LogisticRegression`. Despite its name, logistic regression is a classifier that returns confidence estimates for each class.  \n",
    "\n",
    "As with the regression example, `sklearn` provides toy datasets for classification problems. We'll use the Iris dataset, which is a dataset that uses features of the flower (e.g. sepal length, width) to predict which kind of Iris it is.  \n",
    "  \n",
    "In the following exercise, the following:  \n",
    "1) Load the iris dataset  \n",
    "2) Try to visualize the dataset. Plot one feature vs. another as a pointplot\n",
    "3) Initialize a logistic regression classifier (AKA \"logit\").  \n",
    "4) Perform 5-fold cross-validation  \n",
    "5) Print the mean performance across the folds  \n",
    "6) Plot the distribution  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant modules\n",
    "import sklearn.datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "#help(sklearn.datasets.load_iris)\n",
    "iris_data, iris_label = sklearn.datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# Examine our data\n",
    "# What is the shape of our data? (150, 4)\n",
    "# Visualize it by plotting one feature vs. another\n",
    "\n",
    "# How many unique labels are in the dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Split into test-train\n",
    "\n",
    "\n",
    "# Initialize logit\n",
    "logit = LogisticRegression(solver='liblinear')  # liblinear helps with smaller datasets\n",
    "\n",
    "### Get score list using cross_val_score\n",
    "# score_list = \n",
    "\n",
    "print(score_list)\n",
    "print(np.mean(score_list))\n",
    "plt.bar(np.arange(5), score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) What 'score' is being plotted? Is the plot informative?  \n",
    "  \n",
    "2) Are the classes equally-difficult to predict? Are some harder to distinguish than others?  \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Example\n",
    "\n",
    "For a problem with C classes, a _confusion matrix_ is a C-by-C matrix that functions like a bar plot. For each sample, it looks at the point (prediction, label) and adds it to that bin in the matrix. The full matrix is created by summing all available samples. Correct predictions are along the diagonal (prediction == label), and you can easily identify which classes are leading to more errors.\n",
    "\n",
    "Using a logistic regression classifier, plot the confusion matrix the data we generated above.\n",
    "\n",
    "Steps:\n",
    "1) Look at the confusion matrix documentation. What arguments are needed?  \n",
    "2) Create a LogisticRegression model.  \n",
    "3) Fit the model to the training data.  \n",
    "4) Get the model predictions using `mdl.predict(data)`  \n",
    "5) Plot the confusion matrix. You can use either plt.imshow(conf) to plot the matrix, or use the function `sklearn.metrics.plot_confusion_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the confusion matrix function\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# Initialize and train the model\n",
    "mdl = LogisticRegression(solver='liblinear')\n",
    "mdl.fit(iris_data_train, iris_label_train)\n",
    "\n",
    "### Use the confusion matrix plot function:\n",
    "\n",
    "\n",
    "## Alternatively, if you want to do something with the confusion matrix values:\n",
    "# Get predictions\n",
    "#pred = mdl.predict(iris_data_test)\n",
    "\n",
    "# Generate confusion matrix and plot!\n",
    "#conf = confusion_matrix(y_pred=pred, y_true=iris_label_test)\n",
    "#plt.imshow(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see where the errors are coming from: samples that are labeled as '0' are correctly predicted. It looks like the last two categories are sometimes confused for one another.  \n",
    "If we hadn't evaluated the test set, we could go back and modify our input or our model so that it considered those two categories more important. This might mean reweighting samples, or oversampling those categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification\n",
    "\n",
    "In a lot of contexts, our problem is one of binary classification:  \n",
    "- Is the patient sick?  \n",
    "- Does this MRI have evidence of a tumour?  \n",
    "  \n",
    "When evaluating binary classifiers, you're likely to see some terminology pop up regularly. We'll go over some of these terms:  \n",
    "  \n",
    "- **Positive** and **Negative**  \n",
    "Binary classifiers use 'negative' and 'positive' to differentiate between two classes.\n",
    "  \n",
    "- **True Positive** and **True Negative**.  \n",
    "\"True\" refers to the correctness of the prediction relative to a known label. \"True Positive\" means that the model predicted class 1, and the true label was also class 1.  \n",
    "  \n",
    "- **False Positive** and **False Negative**  \n",
    "\"False\" refers to the incorrectness of the prediction.  \"False Positive\" means that the model predicted class 1, but the true label was class 0.  \n",
    "\"False Negative\" means that the model predicted class 0, but the true label was class 1.  \n",
    "  \n",
    "![Binary Confusion Matrix](img/binary_conf_matrix.png)  \n",
    "\n",
    "- **Accuracy**  \n",
    "Ratio between the correct predictions and all predictions:\n",
    "\\begin{equation}\n",
    "{TP + TN} \\over {TP + TN + FP + FN}\n",
    "\\end{equation}  \n",
    "Accuracy reflects the fraction of correct predictions. \"How often is our model correct?\"  \n",
    "  \n",
    "- **Sensitivity**  \n",
    "Ratio between True Positives and all positive labels (TP + FN):  \n",
    "\\begin{equation}\n",
    "    {TP} \\over {TP + FN}  \n",
    "\\end{equation}\n",
    "Sensitivity reflects a model's ability to detect a positive case. \"If the patient is sick, how well do we predict it?\"  \n",
    "  \n",
    "- **Specificity**  \n",
    "Ratio between True Negatives and all negative labels (TN + FP):  \n",
    "\\begin{equation}\n",
    "    {TN} \\over {TN + FP}\n",
    "\\end{equation}\n",
    "Specificity reflects a model's ability to detect a negative case. \"If the patient is healthy, how well do we predict it?\"  \n",
    "  \n",
    "- **Precision**  \n",
    "Ratio between True Positives and all _predicted_ positives:  \n",
    "\\begin{equation}\n",
    "    {TP} \\over {TP + FP}\n",
    "\\end{equation}  \n",
    "Precision reflects how often a model is correct when it makes a positive prediction. \"If the model predicts a patient is sick, what are the chances the patient is sick?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a model is trained, all of these metrics will initially increase as the general proportion of accurate predictions (TP and TN) will increase. After seeing sufficient data, there is usually a tradeoff between sensitivity and specificity.  \n",
    "Increasing a model's affinity for making _positive_ predictions will improve its sensitivity; sensitivity is not penalized for creating false positives.  \n",
    "Similarly, increasing a model's affinity for making _negative_ predictions will improve its specificity; specificity is not penalized for creating false negatives.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example for binary classification\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# help(make_blobs)\n",
    "x_data, y_label = make_blobs(n_samples=100, n_features=2, centers=[[0,1], [0,-1]])\n",
    "\n",
    "plt.plot(x_data[y_label==0,0], x_data[y_label==0, 1], 'b.')\n",
    "plt.plot(x_data[y_label==1,0], x_data[y_label==1, 1], 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifier Example 1\n",
    "\n",
    "Looking at the previous plot, we see that our data overlaps but that it can be separated by a horizontal line. If we classify everything above the line as '0', and everything below the line as '1', we turn our horizontal line into a classifer.  \n",
    "In this exercise, vary the position of the horizontal line from `-1` to `1`, classify the data into 0 or 1, and plot the following metrics vs. position:  \n",
    "- Accuracy  \n",
    "- Sensitivity  \n",
    "- Specificity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    cls = classification_report(y_true, y_pred, output_dict=True)\n",
    "    return cls['1']['recall']\n",
    "def specificity(y_true, y_pred):\n",
    "    cls = classification_report(y_true, y_pred, output_dict=True)\n",
    "    return cls['0']['recall']\n",
    "\n",
    "# NOTE:\n",
    "# Logical indexing\n",
    "# Define some easy-to-recognize values\n",
    "x = np.arange(10)\n",
    "y = np.arange(30,40)\n",
    "print('x: ' + str(x))\n",
    "print('y: ' + str(y))\n",
    "# Python lets us use 'True' and 'False' values to select entries.\n",
    "# If we want all values of 'x' greater than 4, we can use:\n",
    "x_greater_4 = x[x>4]\n",
    "print('x_greater_than_4: ' + str(x_greater_4))\n",
    "\n",
    "# Similarly, if we want to select all values of 'x' when 'y' is less than 33:\n",
    "x_when_y_lt_33 = x[y<33]\n",
    "print('x_when_y_less_than_33: ' + str(x_when_y_lt_33))\n",
    "\n",
    "# Lastly, we can assign values using this indexing:\n",
    "x[x>6] = 6\n",
    "print('new x: ' + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertical position of points are determined by x[:,1]\n",
    "# For all samples with a position above p, set y_pred to 0\n",
    "#  otherwise, set y_pred to 1\n",
    "# vary p according to np.linspace(-1,1,20)\n",
    "\n",
    "# x_data, y_label = make_blobs(n_samples=100, n_features=2, centers=[[0,1], [0,-1]])\n",
    "\n",
    "###########\n",
    "# Compute metrics and store them\n",
    "y_pred = np.zeros(y_label.shape)\n",
    "\n",
    "sens_list = []\n",
    "spec_list = []\n",
    "acc_list = []\n",
    "p_range = np.linspace(-1,1,20)\n",
    "\n",
    "for p in p_range:\n",
    "    ## Define your predictions according to their position x_data[:,1] and whether they are above or below\n",
    "    ## a value ('p')\n",
    "    ## Hint: Use logical indexing\n",
    "    # y_pred\n",
    "    # y_pred\n",
    "    \n",
    "    \n",
    "    sens = \n",
    "    spec = \n",
    "    acc = \n",
    "    \n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "    acc_list.append(acc)\n",
    "\n",
    "###########    \n",
    "    \n",
    "    \n",
    "plt.plot(p_range, sens_list, 'b.-')\n",
    "plt.plot(p_range, spec_list, 'kx-')\n",
    "plt.plot(p_range, acc_list, 'ro-')\n",
    "plt.legend(['Sensitivity', 'Specificity', 'Accuracy'])\n",
    "plt.xlabel('Position of horizontal line')\n",
    "\n",
    "# Create point plot again for easy comparison\n",
    "plt.figure()\n",
    "plt.plot(x_data[y_label==0,0], x_data[y_label==0, 1], 'b.')\n",
    "plt.plot(x_data[y_label==1,0], x_data[y_label==1, 1], 'rx')\n",
    "plt.legend(['Class 0','Class 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. What is the best position for the dividing line?**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification Example 2\n",
    "The previous example showed how three metrics varied relative to one another. The data that was generated was a little unusual for medical contexts, since there were roughly the same number of samples for one class as another. In other words, we had a _balanced_ dataset.  \n",
    "If you examine the metric plot you created, it's hard to see why 'accuracy' as a metric would make a lot of people wary of results. Qualitatively, it seemed to peak right where sensitivity and specificity are balanced. Let's look at what happens to the same metrics when we have an _unbalanced_ dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "### Uneven sample size\n",
    "n_samples_0 = 100  # Samples for class 0\n",
    "n_samples_1 = 500  # Samples for class 1\n",
    "###\n",
    "\n",
    "# Create our blobs\n",
    "x_data_0, y_label_0 = make_blobs(n_samples=n_samples_0, n_features=2, centers=[[0,1]])\n",
    "x_data_1, y_label_1 = make_blobs(n_samples=n_samples_1, n_features=2, centers=[[0,-1]])\n",
    "\n",
    "# Put our data in single variable\n",
    "x_data = np.zeros((n_samples_0 + n_samples_1, 2))\n",
    "x_data[:n_samples_0, :] = x_data_0\n",
    "x_data[n_samples_0:, :] = x_data_1\n",
    "\n",
    "# Put our labels in single variabls\n",
    "y_label = np.zeros((n_samples_0 + n_samples_1,), dtype=np.int64)\n",
    "y_label[:n_samples_0] = 0\n",
    "y_label[n_samples_0:] = 1\n",
    "#\n",
    "\n",
    "#######\n",
    "# Put your metric code from above here if you want a direct comparison\n",
    "y_pred = np.zeros(y_label.shape)\n",
    "\n",
    "sens_list = []\n",
    "spec_list = []\n",
    "acc_list = []\n",
    "p_range = np.linspace(4,-4,20)\n",
    "for p in p_range:\n",
    "    y_pred[x_data[:,1] > p] = 0\n",
    "    y_pred[x_data[:,1] <= p] = 1\n",
    "    sens = sensitivity(y_label, y_pred)\n",
    "    spec = specificity(y_label, y_pred)\n",
    "    acc = accuracy_score(y_label, y_pred)\n",
    "    \n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "    acc_list.append(acc)\n",
    "########\n",
    "\n",
    "\n",
    "plt.plot(p_range, sens_list, 'b.-')\n",
    "plt.plot(p_range, spec_list, 'kx-')\n",
    "plt.plot(p_range, acc_list, 'ro-')\n",
    "plt.legend(['Sensitivity', 'Specificity', 'Accuracy'])\n",
    "plt.xlabel('Position of horizontal line')\n",
    "\n",
    "# Create point plot again for easy comparison\n",
    "plt.figure()\n",
    "plt.plot(x_data[y_label==0,0], x_data[y_label==0, 1], 'b.')\n",
    "plt.plot(x_data[y_label==1,0], x_data[y_label==1, 1], 'rx')\n",
    "plt.legend(['Class 0','Class 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced sets are more common in medical contexts than balanced ones, and the explanation is intuitive. There are more people without a disease than those with it. Without examining the dataset, reporting a model's accuracy can be misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification Example 3 - ROC and AUC\n",
    "One of the questions you may be asking yourself is, \"If selecting the best model depends on the context, how can we compare models?\"  \n",
    "If we're proposing to use a model for a certain problem, we don't know whether other people would even agree with our metric. Having a False Negative might seem extremely bad from our point of view, but the Coalition of Evil Doctors might disagree.  \n",
    "There are measures that help us work around that problem. The question of context boils down to asking, \"When the truth is X, how often does my model predict it?\" and then balancing between `X=0` and `X=1`. We can provide that answer as a plot, known as a receiver operating characteristic curve (\"ROC curve\").  \n",
    "  \n",
    "The ROC curve considers two metrics: the True Positive Rate (aka sensitivity), and the False Positive Rate (aka 1-specificity).  \n",
    "In this exercise, do the following:  \n",
    "1) Initialize a logistic regression classifier.  \n",
    "2) Create two data blobs (balanced is fine). Visualize them.  \n",
    "3) Train your classifier.  \n",
    "4) Plot the ROC curve using `sklearn.metrics.plot_roc_curve`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "# Initialize\n",
    "logit = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Create blobs\n",
    "x_data, y_labels = make_blobs(n_samples=500, n_features=2, centers=[[1,1],[-0.75,-1]])\n",
    "x_data_train, x_data_test, y_label_train, y_label_test = train_test_split(x_data, y_labels, train_size=0.6)\n",
    "# Visualize\n",
    "plt.figure()\n",
    "plt.plot(x_data[y_labels==0, 0], x_data[y_labels==0, 1], 'bo')\n",
    "plt.plot(x_data[y_labels==1, 0], x_data[y_labels==1, 1], 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "logit.fit(x_data_train, y_label_train)\n",
    "\n",
    "# Examine plot_roc_curve documentation:\n",
    "help(plot_roc_curve)\n",
    "\n",
    "# Plot the curve\n",
    "plot_roc_curve(logit, x_data_test, y_label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the curve tell you? Can you determine what the 'best' classifier would be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
