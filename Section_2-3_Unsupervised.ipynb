{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "So far, we've used data where each sample had a label associated with it. Models that require labels use _supervised_ learning. These models made predictions based on data, and when they made incorrect predictions we informed the models how to correct themselves. That is, we supervise the models' learning and show them how to fix their mistakse.  \n",
    "  \n",
    "What about datasets where we don't have labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x_data, y_labels = make_blobs(n_samples=200, n_features=2)\n",
    "plt.plot(x_data[:,0], x_data[:,1], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without additional information, would you be able to classify a point based on its position?  \n",
    "\n",
    "There are algorithms that can be used to create labels for unlabeled data. The algorithms look at the data, identify which points seem similar to others, and apply labels to data which is 'similar enough'. The definition of similarity changes across algorithms. We'll look at `KMeans` in the next example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering via KMeans\n",
    "\n",
    "In this example, we're going to examine how to automatically apply labels to unlabeled data. We'll use `make_blobs` to generate data, then `KMeans` to try to classify the generated points.  \n",
    "  \n",
    "1) Generate data using `make_blobs`. Store the labels in a variable for later. Plot the blobs.  \n",
    "2) Initialize `sklearn.cluster.KMeans`. Initialize with a number of clusters `n_clusters`  \n",
    "3) Fit your KMeans object to your data. Note that since this is unsupervised, you don't fit the labels. i.e. fit(X) instead of fit(X,Y)  \n",
    "4) Plot the predicted labels. Compare to the plot from (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Make blobs\n",
    "x_data, y_label, centers = make_blobs(n_samples=100, n_features=2, return_centers=True)\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(x_data[:,0], x_data[:,1], '.')\n",
    "\n",
    "### Initialize kmeans object\n",
    "# km = \n",
    "\n",
    "# Fit kmeans\n",
    "\n",
    "# Get predictions from kmeans\n",
    "pred = \n",
    "\n",
    "plt.figure()\n",
    "# Create plot for the different blobs:\n",
    "for label in np.unique(km.labels_):\n",
    "    # plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you initialize your KMeans with a different number of clusters?  \n",
    "What happens if you have two clusters close together?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Space Reduction\n",
    "\n",
    "With large-dimensional data, you may find yourself with more data than information. A common way to do feature space reduction is to use methods like Principal Components Analysis (PCA).  \n",
    "PCA generates the set of eigenvectors for the covariance matrix of your data. More simply: it generates samples which explain variance in the data. By using data that explains most of the variance, we can reduce the number of features without sacrificing the information content. As with clustering, PCA is unsupervised and doesn't consider data labels when computing variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Example\n",
    "  \n",
    "In this example, we'll use PCA to reduce data from 4 dimensions to 2 in order to visualize data. Remember the Iris dataset that we classified using logistic regression? It had four dimensions: sepal width and height, petal width and height. Use PCA to reduce the number of features to 2, and plot the distributions for each class of iris.  \n",
    "  \n",
    "1) Load the iris dataset.  \n",
    "2) Initialize a PCA object.  \n",
    "3) Fit PCA to the Iris dataset.  \n",
    "4) Transform the Iris dataset using the PCA components. Plot them; use different symbols/colors to identify the different classes.  \n",
    "5) Initialize a logistic regression classifier.  \n",
    "6) Fit the classifier to the transformed data. How does the model performance compare to the classifier we had earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iris_data, iris_label = load_iris(return_X_y=True)\n",
    "iris_data_train, iris_data_test, iris_label_train, iris_label_test = train_test_split(iris_data,\n",
    "                                                                                     iris_label, train_size=0.6)\n",
    "### Initialize PCA\n",
    "\n",
    "### Use the .fit method to fit, then .transform to transform data into PC space.\n",
    "# (Or use .fit_transform; it does both in one step)\n",
    "# iris_recast_train = \n",
    "\n",
    "for i in range(3):\n",
    "    # Generate Plot\n",
    "    plt.plot(iris_recast_train[iris_label_train==i,0], iris_recast_train[iris_label_train==i, 1], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize and fit logistic regression\n",
    "\n",
    "### Evaluate on test set\n",
    "\n",
    "# Print the score of the logistic classifier\n",
    "\n",
    "\n",
    "\n",
    "### Evaluate on non-transformed set:\n",
    "# Initialize and fit new classifier to non-PCA data\n",
    "\n",
    "# Print the score fo the logistic classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the performance compare between the different sets of features?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
